# -*- coding: utf-8 -*-
"""Redes Neuronales.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14mf_zQmJ-oRhhXjlIhX-TZw8nWxeUDx3

---
# **REDES NEURONALES ARTIFICIALES**  
**Formato IEEE – Ingeniería Electrónica**  
**Autor:** David Arias, Keneth Montiel.  
**Fecha:** Septiembre 2025  

---
---

---
## **Introducción**

Las **Redes Neuronales Artificiales** (RNA) representan una de las herramientas más poderosas y versátiles de la inteligencia artificial moderna.

En un mundo donde la información crece de forma exponencial, las RNA permiten descubrir relaciones ocultas, predecir comportamientos y automatizar decisiones, desde el reconocimiento de voz y de imágenes, hasta la conducción autónoma y la detección temprana de enfermedades.

La idea fundamental es **aproximar funciones** y la capacidad de aprender representaciones jerárquicas hace que sean la base del *deep learning* moderno.

---
---

---
## **1. ¿Qué es una Red Neuronal Artificial?**

Una RNA es un **sistema de procesamiento distribuido** inspiradas en la estructura y el funcionamiento del cerebro humano, estas redes están formadas por capas de neuronas interconectadas que procesan información de manera no lineal, aprendiendo patrones complejos a partir de datos masivos.

Lo fascinante de estas redes no radica solo en su capacidad de imitar ciertos procesos cognitivos, sino en su habilidad para adaptarse. A diferencia de los algoritmos tradicionales, una RNA ajusta de forma automática sus parámetros internos —pesos y sesgos— mediante un proceso iterativo de entrenamiento, mejorando su desempeño cada vez que se expone a más datos.

Las RNA más comunes tienen al menos tres capas:

1. **Capa de entrada**: recibe los datos de entrada.  
2. **Capas ocultas**: hacen transformaciones lineales seguidas de activaciones no lineales.  
3. **Capa de salida**: produce la predicción.

El flujo de datos (en una red feedforward típica) es:

$$
\mathbf{x} \; \rightarrow \; \text{(Capa Entrada)} \; \rightarrow \; \text{(Capa Oculta(s))} \; \rightarrow \; \text{(Capa Salida)} \; \rightarrow \; \hat{y}
$$

Durante el entrenamiento se usa retropropagación (backpropagation) para ajustar los pesos, minimizando una función de costo.

---
---

---
## **2. ¿Cómo funciona? Arquitectura y flujo de datos**

- **Propagación hacia delante**

Las redes neuronales funcionan tomando una media ponderada más un término de sesgo y aplicando una función de activación para añadir una transformación no lineal. En la formulación de la media ponderada, cada peso determina la importancia de cada característica (es decir, cuánto contribuye a predecir la salida).

<div align="center" style="font-size:135%">
$$
z = \sum_{i=1}^{n} w_i x_i + b,
\qquad
a = \phi(z)
$$
</div>

![Group-17.png](data:image/png;base64,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)

*Fuente principal:* Nico Klingler, *Artificial Neural Network: Everything You Need to Know*, Viso.ai (2023).

---
---
"""

import matplotlib.pyplot as plt
import networkx as nx

G = nx.DiGraph()
layers = [3,4,2]  # nodos por capa
pos = {}
k = 0
for i,n in enumerate(layers):
    for j in range(n):
        pos[k] = (i, j - n/2)
        k += 1
edges=[]
for i in range(layers[0]):
    for j in range(layers[0], layers[0]+layers[1]):
        edges.append((i,j))
for i in range(layers[0], layers[0]+layers[1]):
    for j in range(layers[0]+layers[1], sum(layers)):
        edges.append((i,j))
G.add_edges_from(edges)
nx.draw(G, pos, with_labels=False, node_size=500, node_color="skyblue")
plt.title("Ejemplo de arquitectura 3-4-2")
plt.show()

"""---
## **2.1 Funciones de Activación**
Las funciones de activación introducen **no linealidad**, elemento clave para que la red modele relaciones complejas.
<div align="center" style="font-size:135%">
La elección de \(\phi\) influye directamente en la velocidad de entrenamiento y el rendimiento final.
</div>

Algunas funciones de activación comunes:
* **Sigmoide**: comprime cualquier valor real a un rango entre 0 y 1, con forma de “S”.

  $$
  \sigma(z)=\frac{1}{1+e^{-z}}
  $$
Útil cuando se necesitan probabilidades, pero puede provocar gradientes muy pequeños (“desvanecimiento”).

* **ReLU** (*Rectified Linear Unit*): deja pasar tal cual los valores positivos y convierte en 0 los negativos.

  $$
  \phi(z)=\max(0,z)
  $$
  Es la más usada en capas ocultas porque es simple y acelera el aprendizaje, aunque puede “apagar” neuronas si quedan en cero.

* **Tangente Hiperbólica**: transforma los valores a un rango entre -1 y 1, también con forma de “S”, pero centrada en cero.

  $$
  \tanh(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
  $$
  Mantiene un balance entre positivos y negativos, lo que ayuda a que el modelo aprenda más rápido en casos donde la media está cerca de cero lo que lo hace mas optimo que utilizar la sigmoide.

* **Softmax:** Convierte un **vector de valores** (logits) en una **distribución de probabilidad.**

  $$
  \text{softmax}(z_i)=\frac{e^{z_i}}{\sum_j e^{z_j}}
  $$
  Útil en la capa de salida para **clasificación multiclase**, Cada valor queda entre 0 y 1 y la suma total es 1.

---
---

"""

import numpy as np
import matplotlib.pyplot as plt

# Rango de entrada
x = np.linspace(-6, 6, 400)

# Funciones de activación
sigmoid = 1 / (1 + np.exp(-x))
relu = np.maximum(0, x)
tanh = np.tanh(x)
softmax = np.exp(x) / (np.exp(x) + np.exp(0))   # softmax para dos clases [x,0]

# Crear figura con subplots (2 filas, 2 columnas)
fig, axs = plt.subplots(2, 2, figsize=(10, 8))

# 1. Sigmoide
axs[0,0].plot(x, sigmoid, color="blue")
axs[0,0].set_title("Sigmoide")
axs[0,0].grid(True)

# 2. ReLU
axs[0,1].plot(x, relu, color="green")
axs[0,1].set_title("ReLU")
axs[0,1].grid(True)

# 3. Tanh
axs[1,0].plot(x, tanh, color="red")
axs[1,0].set_title("Tangente Hiperbólica")
axs[1,0].grid(True)

# 4. Softmax (2 clases)
axs[1,1].plot(x, softmax, color="purple")
axs[1,1].set_title("Softmax (probabilidad clase 1)")
axs[1,1].grid(True)

# Etiquetas comunes
for ax in axs.flat:
    ax.axhline(0, color="black", lw=0.8, ls="--")
    ax.axvline(0, color="black", lw=0.8, ls="--")
    ax.set_xlabel("x")
    ax.set_ylabel("Salida")

plt.tight_layout()
plt.show()

"""---
## 3. Cómo asignar el valor de los pesos y los sesgos
La inicialización del peso es el primer componente de la arquitectura de la red neuronal. Los pesos iniciales los fijamos para definir el punto de partida del proceso de optimización del modelo de red neuronal.

- **Inicialización cero:** significa que los pesos se inicializan como cero. No es una buena solución, ya que nuestra red neuronal no rompería la simetría: no aprendería. Siempre que se utilice un valor constante para inicializar los pesos de una red neuronal, podemos esperar que su rendimiento sea deficiente, ya que todas las capas aprenderán lo mismo.

- **La inicialización aleatoria:** rompe la simetría, lo que significa que es mejor que la inicialización cero, pero algunos factores pueden determinar la calidad general del modelo. Por ejemplo, si los pesos se inicializan aleatoriamente con valores y se aplica una función de activación sigmoide el resultado es un valor próximo a uno, mientras que si se inicializan aleatoriamente con valores pequeños aplicando una función sigmoide se obtendrá un valor próximo a cero, lo que también ralentiza el aprendizaje.


---
---

---
## 3. Cómo asignar el valor de los pesos y los sesgos
- **Inicialización de Xavier/Glorot**
Es habitual ver este enfoque de inicialización siempre que se aplica una función de activación tanh o sigmoide a la media ponderada. El objetivo de esta técnica de inicialización es mantener igual la varianza en toda la red para evitar el problema de explosión o desvanecimiento de gradiente.

- **Inicialización de He/Kaiming**
La diferencia con respecto a la anterior es que se utiliza un factor de escala diferente para los pesos que tiene en cuenta la no linealidad de las funciones de activación. Por tanto, cuando se utiliza la función de activación ReLU en las capas, la inicialización de He es el enfoque recomendado.

---
---

---
# 4. Terminos a tener en cuenta
**Hiperparámetros:**
- Son valores definidos antes del entrenamiento que determinan la arquitectura y el proceso de aprendizaje de una red neuronal artificial (RNA).  
- Afectan la **capacidad de generalización**, la **velocidad de convergencia** y el **costo computacional**.  

Su ajuste cuidadoso es esencial para obtener un rendimiento óptimo.

**Neuronas de Entrada:**
- Representan las **variables de entrada** del conjunto de datos.  
- Cada característica normalizada o codificada se mapea a **una neurona**.  
- **Ejemplo:** Imágenes de 28×28 píxeles ⇒ $$28 \times 28 = 784--->neuronas-de-entrada. $$
- Una definición precisa evita pérdida de información y asegura que la red reciba todos los patrones relevantes.

---
---

---
# 4. Terminos a tener en cuenta
**Capas Ocultas**

Son los “procesadores intermedios” de una red neuronal, reciben la información de las entradas, la transforman y codifican paso a paso para que la capa de salida pueda tomar una decisión o hacer una predicción.

Su papel, a grandes rasgos es:
1. **Combinación lineal:**
Cada neurona de la capa calcula una suma ponderada de las salidas de la capa anterior más un sesgo.

2. **No linealidad con función de activación:**
Ese valor z se pasa por una función de activacion para introducir no linealidad, lo que permite que la red aprenda patrones complejos y no solo relaciones lineales.

3. **Extracción de características:**
A medida que los datos atraviesan varias capas, cada una aprende representaciones más abstractas, las primeras capas pueden detectar patrones simples (bordes en una imagen, por ejemplo).
Las posteriores combinan esas características para reconocer formas, objetos o relaciones de alto nivel.
- **Neuronas por capa:**  
  - Regla empírica:  
$$
    n_{\text{hidden}} \approx \frac{n_{\text{input}} + n_{\text{output}}}{2}
$$  
  - Aumentar gradualmente si existe *underfitting*, reducir si hay *overfitting*.  
- Más capas y neuronas aumentan capacidad, pero también riesgo de sobreajuste y mayor tiempo de entrenamiento.

---
---

---
# 4. Terminos a tener en cuenta
**Neuronas de Salida:**
son las que entregan el resultado final del modelo después de que la información ha pasado por todas las capas ocultas. Su funcionamiento, de forma sencilla es:

1. **Reciben entradas:** cada neurona de salida toma los valores ya procesados por la última capa oculta.
2. **Aplican una función de activación según el tipo de tarea:**

- **Clasificación binaria:** se usa normalmente sigmoide, que da una probabilidad entre 0 y 1.

- **Clasificación multiclase:** se usa softmax, que produce una probabilidad para cada clase y todas suman 1.

- **Regresión:** a menudo se deja sin activación (lineal) para poder dar cualquier valor real.

3. **Entregan la predicción:** el valor activado es la salida de la red, por ejemplo, la clase más probable, la probabilidad de un evento o un número continuo.

En pocas palabras, las neuronas de salida son el “traductor final” que convierte todo el cálculo interno en un resultado interpretable según el problema que quieras resolver. Una configuración adecuada garantiza resultados interpretables y estables.

---
---

---
# 4. Terminos a tener en cuenta
**Tamaño de Lote** (*Batch Size*)
Es el “paquete de datos” que la red ve antes de cada ajuste; elegirlo balancea velocidad, memoria y calidad de aprendizaje, indica cuántos ejemplos de entrenamiento procesa la red antes de actualizar sus pesos en una iteración.

- Se divide el conjunto de datos en lotes del tamaño elegido (por ejemplo, 32 muestras por lote).

- Para cada lote, la red hace un forward pass (calcula la salida), mide el error y hace un backpropagation para ajustar los pesos.

- Cuando se procesan todos los lotes, se completa una época.

**Efecto del tamaño:**

Lote pequeño (p.ej. 16 o 32): actualizaciones más frecuentes, aprendizaje más ruidoso pero mejor capacidad de generalización y menor uso de memoria.

Lote grande (p.ej. 256 o más): entrenamiento más estable y rápido por iteración, pero requiere más memoria y puede atascarse en mínimos locales.

**Escalado de Funciones**
- Normalizar o estandarizar las entradas acelera la convergencia del gradiente.  
- Crítico con activaciones **sigmoide** o **tanh**, que saturan ante valores grandes.  
- Métodos recomendados:  
  - **Normalización min–max:** $$(x - x_{min})/(x_{max}-x_{min})$$  
  - **Estandarización z-score:** $$(x - \mu)/\sigma$$

---
---

---
# 4. Terminos a tener en cuenta
**Tasa de Aprendizaje**

Es el control de velocidad del entrenamiento; demasiado alta causa inestabilidad, demasiado baja vuelve el aprendizaje interminable. Es un número que controla qué tan grandes son los pasos que da el optimizador al ajustar los pesos en cada actualización.
$$(\eta)$$
  
**Optimizadores**

El optimizador es el algoritmo que decide cómo mover los pesos en cada actualización, y su elección determina tanto la **velocidad** para llegar a un buen mínimo como la **estabilidad** del entrenamiento.
- **SGD:** ajusta los pesos usando el gradiente promedio de cada mini-lote, es simple y confiable, pero la velocidad depende mucho de elegir bien la tasa de aprendizaje $$\eta$$  
- **Adam:** combina momentum (promedio de gradientes pasados) y una adaptación individual de la tasa de aprendizaje para cada peso, suele converger rápido y requiere menos ajuste manual, por eso es la opción inicial más común.  
- **RMSProp / Adagrad:** ajustan la tasa de aprendizaje en función de la variabilidad de cada parámetro, dándoles pasos más pequeños a los que cambian mucho, van bien en datos secuenciales o muy dispersos, como texto o series de tiempo.  

---
---

---
## 5. Aplicaciones de las Redes Neuronales

Las RNA se emplean en una amplia gama de áreas:

* **Visión por computadora**: reconocimiento facial, clasificación y segmentación de imágenes, detección de objetos en tiempo real para vehículos autónomos.
* **Procesamiento de lenguaje natural**: traducción automática, asistentes virtuales y chatbots capaces de comprender contexto y generar texto.
* **Diagnóstico médico**: análisis de imágenes radiológicas, detección temprana de tumores y predicción de enfermedades a partir de historiales clínicos.
* **Ingeniería y control**: mantenimiento predictivo, control adaptativo de procesos industriales y optimización de sistemas energéticos.
* **Finanzas**:detección de fraude en transacciones, análisis de riesgo crediticio y predicción de tendencias en series temporales.

Su fortaleza radica en **extraer representaciones complejas** sin necesidad de diseñar manualmente las características.

---
---

---
## 6. Ventajas y Desventajas

**Ventajas**
* Capacidad para modelar **relaciones no lineales** y detectar patrones muy complejos que otros métodos no logran capturar.
* Se adaptan a diferentes tipos de datos (texto, audio, imagen).
* Aprenden representaciones jerárquicas automáticamente, reduciendo la necesidad de diseñar manualmente las características de entrada.

**Desventajas**
* Requieren **grandes cantidades de datos** y potencia computacional.
* Riesgo de **sobreajuste** si no se regula adecuadamente.
* Son modelos de **caja negra**: difícil interpretabilidad.
* Necesitan ajuste cuidadoso de hiperparámetros (arquitectura, tasa de aprendizaje, etc.).

Estas características obligan a equilibrar poder predictivo y costo de implementación.

---
---

---
## 7. Ejemplo en Python + Compración con Regresion Logística


---
---
"""

# Librerías básicas
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

# Modelos clásicos
from sklearn.linear_model import LogisticRegression

# Red neuronal (Keras)
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Reducimos a 30k imágenes de entrenamiento
x_train, _, y_train, _ = train_test_split(x_train, y_train, train_size=30000, stratify=y_train, random_state=42)

# Normalización [0,1]
x_train = x_train.astype("float32")/255.0
x_test  = x_test.astype("float32")/255.0

# Aplanar imágenes 28x28 → 784
x_train = x_train.reshape((x_train.shape[0], -1))
x_test  = x_test.reshape((x_test.shape[0], -1))

model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(x_train, y_train,
                    validation_split=0.2,
                    epochs=5,
                    batch_size=64,
                    verbose=2)

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"RNA – Exactitud en prueba: {test_acc:.4f}")

log_reg = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial')
log_reg.fit(x_train, y_train)

y_pred_lr = log_reg.predict(x_test)
acc_lr = log_reg.score(x_test, y_test)
print(f"Regresión Logística – Exactitud en prueba: {acc_lr:.4f}")

print("== RNA ==")
y_pred_nn = np.argmax(model.predict(x_test), axis=1)
print(classification_report(y_test, y_pred_nn))

print("\n== Regresión Logística ==")
print(classification_report(y_test, y_pred_lr))

# La red neuronal devuelve probabilidades para cada dígito (10 clases)
prob_nn = model.predict(x_test)

# Convertimos a la clase con mayor probabilidad
y_pred_nn = np.argmax(prob_nn, axis=1)

# Mostramos las primeras 10 predicciones junto con las etiquetas reales
print("Predicciones RNA:", y_pred_nn[:10])
print("Etiquetas reales :", y_test[:10])

# La regresión logística predice directamente la clase
y_pred_lr = log_reg.predict(x_test)

print("Predicciones Regresión Logística:", y_pred_lr[:10])
print("Etiquetas reales                :", y_test[:10])

"""---
## 8. Resultados y Discusión


---
---

---
## 8.1 Matrices de confusión

Cada cuadro muestra cuántas imágenes de cada dígito fueron clasificadas correctamente o de forma errónea.

Azul corresponde a la red neuronal y verde a la regresión logística.

La diagonal representa los aciertos: mientras más intenso el color, mejor el modelo.

Observamos que la red neuronal presenta una diagonal más marcada, lo que indica mayor precisión en casi todos los dígitos.

---
---
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

print("Exactitud RNA :", accuracy_score(y_test, y_pred_nn))
print("Exactitud RL  :", accuracy_score(y_test, y_pred_lr))



# Matrices para cada modelo
cm_nn = confusion_matrix(y_test, y_pred_nn)
cm_lr = confusion_matrix(y_test, y_pred_lr)

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

sns.heatmap(cm_nn, annot=True, fmt="d", cmap="Blues", ax=axes[0])
axes[0].set_title("RNA - Matriz de Confusión")
axes[0].set_xlabel("Predicción")
axes[0].set_ylabel("Etiqueta real")

sns.heatmap(cm_lr, annot=True, fmt="d", cmap="Greens", ax=axes[1])
axes[1].set_title("Regresión Logística - Matriz de Confusión")
axes[1].set_xlabel("Predicción")
axes[1].set_ylabel("Etiqueta real")

plt.tight_layout()
plt.show()

"""---
## 8.2 Curvas de entrenamiento (solo RNA)

La gráfica muestra dos curvas de entrenamiento de una Red Neuronal Artificial (RNA):

Izquierda (Exactitud/Accuracy): porcentaje de aciertos en entrenamiento y validación a lo largo de las épocas.

Derecha (Pérdida/Loss): valor de la función de costo, que mide el error del modelo.

En ambos casos se grafican dos líneas:

Azul: datos de entrenamiento.

Naranja: datos de validación (datos que el modelo no vio durante el entrenamiento, usados para medir generalización).

---
---
"""

plt.figure(figsize=(12,4))

# Exactitud
plt.subplot(1,2,1)
plt.plot(history.history["accuracy"], label="Entrenamiento")
plt.plot(history.history["val_accuracy"], label="Validación")
plt.title("Exactitud RNA")
plt.xlabel("Época")
plt.ylabel("Accuracy")
plt.legend()

# Pérdida
plt.subplot(1,2,2)
plt.plot(history.history["loss"], label="Entrenamiento")
plt.plot(history.history["val_loss"], label="Validación")
plt.title("Pérdida RNA")
plt.xlabel("Época")
plt.ylabel("Loss")
plt.legend()

plt.tight_layout()
plt.show()

"""---
## 8.3 Ejemplos de predicciones

Aquí seleccionamos algunas imágenes al azar. Cada recuadro indica el valor real y las predicciones de ambos modelos.

Pueden notar que en la mayoría de los casos los dos aciertan, pero en dígitos más confusos, como el 5 y el 8, la red neuronal tiende a equivocarse menos.

---
---
"""

import numpy as np

# Elegimos 10 imágenes aleatorias del set de prueba
indices = np.random.choice(len(x_test), 10, replace=False)

plt.figure(figsize=(15,4))
for i, idx in enumerate(indices):
    plt.subplot(2,5,i+1)
    plt.imshow(x_test[idx].reshape(28,28), cmap="gray")
    plt.title(f"Real: {y_test[idx]}\nRNA: {y_pred_nn[idx]}\nRL: {y_pred_lr[idx]}")
    plt.axis("off")
plt.tight_layout()
plt.show()

"""---
## 9. Conclusiones

Las Redes Neuronales Artificiales (RNA) representan más que una técnica de modelado:
son un paradigma de pensamiento computacional que imita, a gran escala, la forma en que el cerebro procesa información.

- Impacto transversal: hoy son la base de avances disruptivos en vehículos autónomos, asistentes virtuales, descubrimiento de fármacos y sistemas de recomendación, transformando industrias enteras.

- Aprendizaje continuo: su fortaleza radica en la capacidad de adaptarse a nuevos datos y contextos, convirtiéndose en sistemas que mejoran con la experiencia.

- Reto de interpretabilidad: a pesar de su éxito, comprender sus decisiones sigue siendo un desafío, impulsando la investigación en IA explicable y métodos de visualización de redes.

- Proyección futura: combinadas con computación cuántica y hardware especializado, se espera que las RNA permitan resolver problemas antes considerados intratables.

<div align="center" style="font-size:120%; font-weight:bold; margin-top:12px;"> Las RNA no solo resuelven problemas de hoy: definen el camino de la inteligencia artificial del mañana.
</div>

---
---

---
## Referencias

[1] OpenAI, Asistencia generada por el modelo GPT-5 de ChatGPT, 2025.

[2] N. Klingler, Artificial Neural Network: Everything You Need to Know, Viso.ai, Jan. 2023.

[3] J. Amat Rodrigo, Redes neuronales con Python, Ciencia de Datos, May. 2021.

[4] S. Haykin, Neural Networks and Learning Machines, 3rd ed., Pearson, 2009.


---
---
"""